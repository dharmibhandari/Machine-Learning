# Project: In the dataset HR Wants to predict wheather a employee leave the stay


# Multivariable classfication

when data contain multiple feature 

# Import Required Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns


# Import Dataset

df = pd.read_csv("logistic_reg.csv")

df.head()

# Statistical Information

df.describe()


# To check null values

df.isnull().sum()


# To check unique values in target column

df['left'].unique()

# Estimated Value of Low income salary has high probabilty of leaving

#manual check if salary affect on employee 
sns.barplot(x = 'left', y = 'salary', data = df)


#feature selection
df.corr(numeric_only=True)
sns.heatmap(df.corr(numeric_only=True), xticklabels=df.corr(numeric_only=True).columns, yticklabels=df.corr(numeric_only=True).columns, annot=True, linewidth=1.5, cmap="YlGnBu")


df.head()

df = pd.get_dummies(df,columns=['salary'], prefix="salary")


df.head()

df.columns

x = df[['satisfaction_level', 'number_project',
        'time_spend_company',
       'promotion_last_5years','salary_high', 'salary_low',
       'salary_medium']]
y = df.left

x.head()

x.shape

y.shape

# Split Tarin and Test Data

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.7,random_state=100)


y_test

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)


pred=model.predict(X_test)


compare=pd.DataFrame({"ACTUAL":y_test,"PREDICTED":pred})

compare.reset_index()

# Model Evulation



#check traing model score
model.score(X_test,y_test)



#cheack  testing model score
model.score(X_train,y_train)


#there is no overfitting and underfitting

y_pred = model.predict(X_test)


# Confusion matrix

from sklearn.metrics import confusion_matrix

con_mat = confusion_matrix(y_test,y_pred)
con_mat

plt.figure(figsize = (10,7))
sns.heatmap(con_mat, annot=True,fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')


con_mat

from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,classification_report

print("Accuracy:",accuracy_score(y_test, y_pred))
print("Precision:",precision_score(y_test, y_pred))
print("Recall:",recall_score(y_test, y_pred))
print("f1_score:",f1_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

y_test.value_counts()

# ROC Curve

The receiver operating characteristic (ROC) curve is another common tool used with
binary classifiers. 

the ROC curve plots the true positive rate (another name for recall) against the false positive rate. 

The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,
which is the ratio of negative instances that are correctly classified as negative. The
TNR is also called specificity. (recall)

from sklearn.metrics import roc_curve,roc_auc_score

y_pred_proba = model.predict_proba(X_test)[::,1]
fpr, tpr, threshold = roc_curve(y_test,  y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.xlabel("False Positive Rate")
plt.ylabel(" Positive Rate")

plt.show()

# gives probability of ech data to be 0 or 1
model.predict_proba(X_test)

model.predict_proba(X_test)[::,1]

